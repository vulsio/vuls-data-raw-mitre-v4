{
  "name": "CVE-2023-29374",
  "seq": "2023-29374",
  "type": "CAN",
  "status": "Candidate",
  "phase": {
    "text": "Assigned",
    "date": "20230405"
  },
  "desc": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method.",
  "refs": [
    {
      "text": "https://github.com/hwchase17/langchain/issues/1026",
      "source": "MISC",
      "url": "https://github.com/hwchase17/langchain/issues/1026"
    },
    {
      "text": "https://github.com/hwchase17/langchain/issues/814",
      "source": "MISC",
      "url": "https://github.com/hwchase17/langchain/issues/814"
    },
    {
      "text": "https://github.com/hwchase17/langchain/pull/1119",
      "source": "MISC",
      "url": "https://github.com/hwchase17/langchain/pull/1119"
    },
    {
      "text": "https://twitter.com/rharang/status/1641899743608463365/photo/1",
      "source": "MISC",
      "url": "https://twitter.com/rharang/status/1641899743608463365/photo/1"
    }
  ],
  "votes": {}
}
